# The Transformer Family

_Attention_ is a mechanism in the neural network that a model can learn to make predictions by selectively attending to a given set of data. The amount of attention is quantified by learned weights and thus the output is usually formed as a weighted average.

_Self-attention_ is a type of attention mechanism where the model makes prediction for one part of a data sample using other parts of the observation about the same sample. Conceptually, it feels quite similar to [non-local means](https://en.wikipedia.org/wiki/Non-local_means). Also note that self-attention is permutation-invariant; in other words, it is an operation on sets.

In this article, author make a great summary of different types of transformers.

[https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html)

